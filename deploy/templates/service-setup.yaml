# This manifest runs a Job whenever the Helm chart is installed or
# upgraded. This Job pulls in an image with configuration to load into
# the F+ services which is specific to deployment decisions made by ACS.
apiVersion: batch/v1
kind: Job
metadata:
  namespace: {{ .Release.Namespace }}
  # Give the job a random id so helm reruns it on helm upgrade.
  name: service-setup-{{ randAlphaNum 8 | lower }}
spec:
  backoffLimit: 9999
  template:
    spec:
      serviceAccountName: service-setup
      restartPolicy: OnFailure
      volumes:
        - name: git-checkouts
          emptyDir: { }
        - name: krb5-conf
          configMap:
            name: krb5-conf
      initContainers:
        - name: service-setup
{{ include "amrc-connectivity-stack.image" (list . .Values.serviceSetup) | indent 10 }}
          env:
            - name: DIRECTORY_URL
              value: http://directory.{{ .Release.Namespace }}.svc.cluster.local
            - name: SERVICE_USERNAME
              value: admin
            - name: SERVICE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: admin-password
                  key: password
            - name: VERBOSE
              value: ALL,!token,!service
            - name: GIT_CHECKOUTS
              value: /data
            - name: SS_CONFIG
              value: {{ .Values.serviceSetup.config | toRawJson | quote }}
            - name: ACS_CONFIG
              value: {{
                dict
                  "organisation"    .Values.acs.organisation
                  "namespace"       .Release.Namespace
                  "domain"          .Values.acs.baseUrl
                  "k8sdomain"       "cluster.local"
                  "secure"          (.Values.acs.secure | ternary "s" "")
                  "realm"           .Values.identity.realm
                  "directory"
                    (include "amrc-connectivity-stack.external-url" 
                      (list . "directory"))
                | toRawJson | quote }}
          volumeMounts:
            - mountPath: /data
              name: git-checkouts
        - name: edge-helm-charts
{{ include "amrc-connectivity-stack.image" (list . .Values.edgeHelm) | indent 10 }}
          env:
            - name: DIRECTORY_URL
              value: http://directory.{{ .Release.Namespace }}.svc.cluster.local
            - name: SERVICE_USERNAME
              value: admin
            - name: SERVICE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: admin-password
                  key: password
            - name: VERBOSE
              value: ALL,!token,!service
            - name: GIT_REPO_PATH
              value: {{ .Values.edgeHelm.repoPath }}
{{ if .Values.acs.schemas.load }}
        - name: load-schemas
{{ include "amrc-connectivity-stack.image" (list . .Values.acs.schemas) | indent 10 }}
          env:
            - name: DIRECTORY_URL
              value: http://directory.{{ $.Release.Namespace }}.svc.cluster.local
            - name: SERVICE_USERNAME
              value: admin
            - name: SERVICE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: admin-password
                  key: password
            - name: VERBOSE
              value: ALL,!token,!service
{{ end }}
      containers:
        - name: restart-mqtt
{{ include "amrc-connectivity-stack.image" (list . .Values.kubectl) | indent 10 }}
          command:
            - /bin/sh
            - -c
            - |
              DEPLOYMENT_NAME=mqtt
              NAMESPACE={{.Release.Namespace}}

              echo "Attempting to restart deployment: $DEPLOYMENT_NAME"

              # Try to rollout restart the deployment
              kubectl rollout restart deployment/$DEPLOYMENT_NAME -n $NAMESPACE

              # Wait and check rollout status
              if kubectl rollout status deployment/$DEPLOYMENT_NAME -n $NAMESPACE --timeout=60s; then
                echo "Deployment restarted successfully!"
                exit 0  # Exit successfully, no pod restart needed
              else
                echo "Deployment restart failed, retrying..."
                exit 1  # Exit with error, pod will restart
              fi
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: service-setup
  namespace: {{ .Release.Namespace }}

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: service-setup-role
  namespace: {{ .Release.Namespace }}
rules:
  - apiGroups: [apps]
    resources: [deployments]
    verbs: [get, patch, update, list, watch]
  - apiGroups: [factoryplus.app.amrc.co.uk]
    resources: [kerberos-keys]
    verbs: [list, get]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: service-setup-binding
  namespace: {{ .Release.Namespace }}
subjects:
  - kind: ServiceAccount
    name: service-setup
    namespace: {{ .Release.Namespace }}
roleRef:
  kind: Role
  name: service-setup-role
  apiGroup: rbac.authorization.k8s.io
